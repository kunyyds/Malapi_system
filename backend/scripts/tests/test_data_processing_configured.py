#!/usr/bin/env python3
"""
é…ç½®åŒ–æ•°æ®å¤„ç†å±‚æµ‹è¯•è„šæœ¬

ç‰¹æ€§ï¼š
1. é›†ä¸­é…ç½®ç®¡ç†
2. ç¯å¢ƒå˜é‡æ”¯æŒ
3. æ™ºèƒ½è·¯å¾„æ£€æµ‹
4. è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
5. æ€§èƒ½ç»Ÿè®¡å’Œåˆ†æ

ä½¿ç”¨æ–¹æ³•ï¼š
1. é»˜è®¤é…ç½®: python test_data_processing_configured.py
2. è‡ªå®šä¹‰é…ç½®: MALAPI_TEST_PARSER_FILES=20 python test_data_processing_configured.py
3. æ˜¾ç¤ºé…ç½®: python test_config.py
"""

import asyncio
import logging
import sys
import json
import os
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®
from test_config import get_config, TestConfig

# é…ç½®æ—¥å¿—
def setup_logging(config: TestConfig):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

    handlers = [
        logging.StreamHandler(),
        logging.FileHandler(config.log_file)
    ]

    logging.basicConfig(
        level=getattr(logging, config.log_level.upper(), logging.INFO),
        format=log_format,
        handlers=handlers
    )

logger = logging.getLogger(__name__)


async def test_file_scanner(config: TestConfig):
    """æµ‹è¯•æ–‡ä»¶æ‰«æå™¨"""
    logger.info("=" * 50)
    logger.info("æµ‹è¯•æ–‡ä»¶æ‰«æå™¨")
    logger.info("=" * 50)

    try:
        from src.parsers.file_scanner import FileScanner

        # åˆ›å»ºæ‰«æå™¨ï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰
        scanner = FileScanner(
            max_workers=config.scanner_max_workers,
            max_depth=config.scanner_max_depth
        )

        # æ™ºèƒ½æ£€æµ‹æµ‹è¯•ç›®å½•è·¯å¾„ï¼ˆä½¿ç”¨é…ç½®çš„å€™é€‰è·¯å¾„ï¼‰
        files_directory = None
        possible_paths = config.get_possible_files_paths()

        for path in possible_paths:
            if path.exists() and path.is_dir():
                files_directory = path
                logger.info(f"æ‰¾åˆ°æµ‹è¯•æ•°æ®ç›®å½•: {files_directory}")
                break

        if not files_directory:
            logger.error(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ç›®å½•ï¼Œå°è¯•äº†ä»¥ä¸‹è·¯å¾„:")
            for path in possible_paths:
                logger.error(f"  - {path}")
            logger.error(f"è¯·ç¡®ä¿ files ç›®å½•å­˜åœ¨ä¸”åŒ…å« manifest.json æ–‡ä»¶")
            return None

        # æ‰«æmanifest.jsonæ–‡ä»¶
        logger.info(f"æ‰«æç›®å½•: {files_directory}")
        scan_result = await scanner.scan_directory(
            root_path=files_directory,
            pattern="manifest",
            recursive=True
        )

        # è¾“å‡ºæ‰«æç»“æœ
        logger.info(f"æ‰«æç»“æœ: {scan_result.get_summary()}")
        logger.info(f"æ‰¾åˆ°æ–‡ä»¶æ•°: {scan_result.get_file_count()}")

        # æ˜¾ç¤ºå‰Nä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨é…ç½®çš„æ˜¾ç¤ºé™åˆ¶ï¼‰
        if scan_result.files:
            logger.info(f"å‰{config.scan_result_display_limit}ä¸ªæ–‡ä»¶:")
            display_limit = min(config.scan_result_display_limit, len(scan_result.files))
            for i, file_path in enumerate(scan_result.files[:display_limit]):
                logger.info(f"  {i+1}. {file_path}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        scanner.print_statistics()

        return scan_result

    except Exception as e:
        logger.error(f"æ–‡ä»¶æ‰«æå™¨æµ‹è¯•å¤±è´¥: {e}")
        logger.error("å¯èƒ½çš„åŸå› :")
        logger.error("  1. ä¾èµ–æ¨¡å—æœªæ­£ç¡®å®‰è£…")
        logger.error("  2. æ–‡ä»¶ç³»ç»Ÿæƒé™é—®é¢˜")
        logger.error("  3. æ¨¡å—å¯¼å…¥è·¯å¾„é”™è¯¯")
        return None


async def test_manifest_parser(scan_result, config: TestConfig):
    """æµ‹è¯•manifestè§£æå™¨"""
    logger.info("=" * 50)
    logger.info("æµ‹è¯•manifestè§£æå™¨")
    logger.info("=" * 50)

    if not scan_result or not scan_result.files:
        logger.warning("æ²¡æœ‰æ–‡ä»¶å¯ä¾›æµ‹è¯•è§£æå™¨")
        return []

    try:
        from src.parsers.manifest_parser import ManifestParser

        # åˆ›å»ºè§£æå™¨ï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰
        parser = ManifestParser(
            strict_mode=config.strict_validation,
            validate_attack_ids=config.validate_attack_ids
        )

        parse_results = []
        test_files_count = min(config.parser_test_files_count, len(scan_result.files))
        test_files = scan_result.files[:test_files_count]

        logger.info(f"æµ‹è¯•è§£æ {test_files_count} ä¸ªæ–‡ä»¶")

        for i, file_path in enumerate(test_files):
            logger.info(f"è§£ææ–‡ä»¶ {i+1}/{test_files_count}: {file_path.name}")

            try:
                parse_result = await parser.parse_file(file_path)

                if parse_result.is_valid:
                    logger.info(f"  âœ… è§£ææˆåŠŸ: {parse_result.data.get('alias', 'N/A')}")
                    logger.info(f"     ATT&CKæŠ€æœ¯: {parse_result.data.get('attck', [])}")
                else:
                    logger.warning(f"  âŒ è§£æå¤±è´¥: {parse_result.get_error_summary()}")

                parse_results.append(parse_result)

            except Exception as e:
                logger.error(f"  âŒ è§£æå¼‚å¸¸: {e}")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        parser_stats = parser.get_statistics()
        success_rate = parser_stats.get('success_rate', 0)
        logger.info(f"è§£æç»Ÿè®¡: æˆåŠŸç‡ {success_rate:.1f}%")
        logger.info(f"å¹³å‡é”™è¯¯æ•°: {parser_stats.get('average_errors_per_file', 0):.1f}")

        return parse_results

    except Exception as e:
        logger.error(f"manifestè§£æå™¨æµ‹è¯•å¤±è´¥: {e}")
        logger.error("å¯èƒ½çš„åŸå› :")
        logger.error("  1. JSONæ ¼å¼é”™è¯¯")
        logger.error("  2. æ•°æ®éªŒè¯è§„åˆ™é—®é¢˜")
        logger.error("  3. æ–‡ä»¶ç¼–ç é—®é¢˜")
        return []


async def test_database_connection(config: TestConfig):
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    logger.info("=" * 50)
    logger.info("æµ‹è¯•æ•°æ®åº“è¿æ¥")
    logger.info("=" * 50)

    try:
        from src.database.connection import async_engine, AsyncSessionLocal

        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        async with AsyncSessionLocal() as session:
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
            from sqlalchemy import text
            result = await session.execute(text("SELECT 1"))
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")

            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ - æ”¯æŒå¤šç§æ•°æ®åº“ç±»å‹
            from src.database.models import MalAPIFunction

            # æ£€æµ‹æ•°æ®åº“ç±»å‹
            db_url = str(async_engine.url).lower()
            logger.info(f"æ•°æ®åº“ç±»å‹: {db_url.split('://')[0] if '://' in db_url else 'unknown'}")

            if 'sqlite' in db_url:
                # SQLite æŸ¥è¯¢
                result = await session.execute(text("SELECT name FROM sqlite_master WHERE type='table' AND name='malapi_functions'"))
            elif 'postgresql' in db_url:
                # PostgreSQL æŸ¥è¯¢
                result = await session.execute(text("SELECT tablename FROM pg_catalog.pg_tables WHERE tablename = 'malapi_functions'"))
            else:
                # é€šç”¨æŸ¥è¯¢ - å°è¯•ç›´æ¥æŸ¥è¯¢è¡¨
                try:
                    result = await session.execute(text("SELECT COUNT(*) FROM malapi_functions LIMIT 1"))
                    table_exists = True
                except Exception:
                    table_exists = False
                logger.info(f"æ•°æ®åº“ç±»å‹æ£€æµ‹: {db_url}")

            if 'sqlite' in db_url or 'postgresql' in db_url:
                table_exists = result.fetchone() is not None

            if table_exists:
                logger.info("âœ… æ•°æ®åº“è¡¨å·²åˆ›å»º")
            else:
                logger.info("ğŸ”§ æ•°æ®åº“è¡¨æœªåˆ›å»ºï¼Œæ­£åœ¨åˆå§‹åŒ–...")
                from src.database.connection import init_db
                await init_db()
                logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        return True

    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        logger.error("è¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:")
        logger.error("  1. æ•°æ®åº“æœåŠ¡æ˜¯å¦è¿è¡Œ")
        logger.error(f"  2. æ•°æ®åº“URLé…ç½®: {config.database_url}")
        logger.error("  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        if config.use_sqlite_fallback and 'postgresql' in str(config.database_url).lower():
            logger.info("ğŸ’¡ å»ºè®®ä½¿ç”¨SQLiteä½œä¸ºæµ‹è¯•æ•°æ®åº“")
        return False


async def test_full_import_workflow(config: TestConfig, scan_result):
    """æµ‹è¯•å®Œæ•´çš„å¯¼å…¥å·¥ä½œæµç¨‹"""
    logger.info("=" * 50)
    logger.info("æµ‹è¯•å®Œæ•´å¯¼å…¥å·¥ä½œæµç¨‹")
    logger.info("=" * 50)

    if not scan_result or scan_result.get_file_count() == 0:
        logger.warning("æ²¡æœ‰æ–‡ä»¶å¯ä¾›å¯¼å…¥æµ‹è¯•")
        return None

    try:
        from src.database.connection import AsyncSessionLocal
        from src.importers.import_manager import ImportManager

        # åˆ›å»ºå¯¼å…¥ç®¡ç†å™¨
        session_factory = AsyncSessionLocal
        manager = ImportManager(session_factory)

        # è®¾ç½®è¿›åº¦å›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.enable_progress_callback:
            def progress_callback(current, total, message):
                percentage = (current / total) * 100
                logger.info(f"è¿›åº¦: {percentage:.1f}% - {message}")

            manager.set_progress_callback(progress_callback)

        # æ™ºèƒ½æ£€æµ‹æµ‹è¯•ç›®å½•è·¯å¾„ï¼ˆå¤ç”¨é…ç½®çš„å€™é€‰è·¯å¾„ï¼‰
        files_directory = None
        possible_paths = config.get_possible_files_paths()

        for path in possible_paths:
            if path.exists() and path.is_dir():
                files_directory = path
                logger.info(f"ä½¿ç”¨æµ‹è¯•æ•°æ®ç›®å½•: {files_directory}")
                break

        if not files_directory:
            logger.error(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ç›®å½•ï¼Œå¯¼å…¥æµç¨‹æµ‹è¯•å–æ¶ˆ")
            return None

        # é™åˆ¶æµ‹è¯•æ–‡ä»¶æ•°é‡ï¼ˆä½¿ç”¨é…ç½®ï¼‰
        original_files = scan_result.files.copy()
        test_files_count = min(config.import_test_files_count, len(original_files))

        scan_result.files = original_files[:test_files_count]
        scan_result.files_found = test_files_count

        # æ‰§è¡Œå¯¼å…¥æµç¨‹
        logger.info(f"å¼€å§‹å®Œæ•´å¯¼å…¥æµç¨‹æµ‹è¯•ï¼ˆ{test_files_count}ä¸ªæ–‡ä»¶ï¼‰...")
        result = await manager.import_from_directory(
            directory_path=files_directory,
            pattern="manifest",
            recursive=True
        )

        # è¾“å‡ºç»“æœ
        logger.info(f"å¯¼å…¥æµç¨‹å®Œæˆ: {result.get_overall_summary()}")

        # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
        stats = result.get_stage_statistics()
        logger.info("è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        for stage, stage_stats in stats.items():
            if isinstance(stage_stats, dict):
                logger.info(f"  {stage}:")
                for key, value in stage_stats.items():
                    logger.info(f"    {key}: {value}")

        # æ‰“å°ç®¡ç†å™¨ç»Ÿè®¡
        manager.print_statistics()

        # æ¢å¤åŸå§‹æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        scan_result.files = original_files
        scan_result.files_found = len(original_files)

        return result

    except Exception as e:
        logger.error(f"âŒ å®Œæ•´å¯¼å…¥æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        logger.error("å¯èƒ½çš„åŸå› :")
        logger.error("  1. æ•°æ®åº“è¿æ¥é—®é¢˜")
        logger.error("  2. å†…å­˜ä¸è¶³")
        logger.error("  3. å¹¶å‘å¤„ç†é—®é¢˜")
        return None


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = get_config()
    setup_logging(config)

    test_start_time = datetime.now()
    logger.info("ğŸš€ å¼€å§‹é…ç½®åŒ–æ•°æ®å¤„ç†å±‚æµ‹è¯•")
    logger.info(f"â° æµ‹è¯•å¼€å§‹æ—¶é—´: {test_start_time}")
    logger.info(f"ğŸ“‚ é¡¹ç›®è·¯å¾„: {config.project_root}")

    # æµ‹è¯•ç¯å¢ƒä¿¡æ¯
    logger.info("=" * 50)
    logger.info("æµ‹è¯•ç¯å¢ƒä¿¡æ¯")
    logger.info("=" * 50)
    logger.info(f"Pythonç‰ˆæœ¬: {sys.version}")
    logger.info(f"å·¥ä½œç›®å½•: {Path.cwd()}")
    logger.info(f"ç¯å¢ƒå˜é‡: MALAPI_TEST_PARSER_FILES={os.getenv('MALAPI_TEST_PARSER_FILES', 'not set')}")

    # æ£€æŸ¥å…³é”®ä¾èµ–
    try:
        import sqlalchemy
        logger.info(f"SQLAlchemyç‰ˆæœ¬: {sqlalchemy.__version__}")
    except ImportError:
        logger.warning("SQLAlchemy æœªå®‰è£… - è¯·æ¿€æ´»malapi-backendç¯å¢ƒ: conda activate malapi-backend")

    try:
        from src.parsers.manifest_parser import ManifestParser
        from src.parsers.file_scanner import FileScanner
        from src.importers.import_manager import ImportManager
        logger.info("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        logger.error(f"âŒ æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        logger.error("è§£å†³æ–¹æ¡ˆ:")
        logger.error("  1. ç¡®ä¿å·²æ¿€æ´»condaç¯å¢ƒ: conda activate malapi-backend")
        logger.error("  2. ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸­è¿è¡Œæµ‹è¯•")
        return

    try:
        # æµ‹è¯•1: æ–‡ä»¶æ‰«æå™¨
        scan_result = await test_file_scanner(config)

        # æµ‹è¯•2: manifestè§£æå™¨
        parse_results = await test_manifest_parser(scan_result, config)

        # æµ‹è¯•3: æ•°æ®åº“è¿æ¥
        db_connected = await test_database_connection(config)

        # æµ‹è¯•4: å®Œæ•´å¯¼å…¥æµç¨‹ï¼ˆä»…åœ¨æ•°æ®åº“è¿æ¥æˆåŠŸä¸”æ‰¾åˆ°æ–‡ä»¶æ—¶æ‰§è¡Œï¼‰
        if db_connected and scan_result and scan_result.get_file_count() > 0:
            import_result = await test_full_import_workflow(config, scan_result)
        else:
            skip_reasons = []
            if not db_connected:
                skip_reasons.append("æ•°æ®åº“è¿æ¥å¤±è´¥")
            if not scan_result or scan_result.get_file_count() == 0:
                skip_reasons.append("æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            logger.warning(f"è·³è¿‡å®Œæ•´å¯¼å…¥æµç¨‹æµ‹è¯•ï¼ˆ{'ã€'.join(skip_reasons)}ï¼‰")

        # æµ‹è¯•å®Œæˆ
        test_end_time = datetime.now()
        test_duration = test_end_time - test_start_time

        logger.info("=" * 50)
        logger.info("ğŸ‰ é…ç½®åŒ–æ•°æ®å¤„ç†å±‚æµ‹è¯•å®Œæˆ")
        logger.info("=" * 50)
        logger.info(f"â° æµ‹è¯•ç»“æŸæ—¶é—´: {test_end_time}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {test_duration.total_seconds():.2f} ç§’")

        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        logger.info("æµ‹è¯•æ€»ç»“:")
        logger.info(f"  æ–‡ä»¶æ‰«æ: {'âœ… æˆåŠŸ' if scan_result else 'âŒ å¤±è´¥'}")
        if scan_result:
            logger.info(f"    æ‰¾åˆ°æ–‡ä»¶æ•°: {scan_result.get_file_count()}")
        logger.info(f"  manifestè§£æ: {'âœ… æˆåŠŸ' if parse_results else 'âŒ å¤±è´¥'}")
        if parse_results:
            successful_parses = sum(1 for r in parse_results if r.is_valid)
            logger.info(f"    è§£ææˆåŠŸæ•°: {successful_parses}/{len(parse_results)}")
        logger.info(f"  æ•°æ®åº“è¿æ¥: {'âœ… æˆåŠŸ' if db_connected else 'âŒ å¤±è´¥'}")

        # é…ç½®ä½¿ç”¨è¯´æ˜
        logger.info("=" * 50)
        logger.info("ğŸ“‹ é…ç½®ä½¿ç”¨è¯´æ˜")
        logger.info("=" * 50)
        logger.info("ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹:")
        logger.info("  MALAPI_TEST_PARSER_FILES=20    # æµ‹è¯•æ–‡ä»¶æ•°é‡")
        logger.info("  MALAPI_TEST_MAX_WORKERS=8      # å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°")
        logger.info("  MALAPI_TEST_DATABASE_URL=sqlite:///./test.db  # æ•°æ®åº“URL")
        logger.info("  MALAPI_TEST_STRICT=true         # ä¸¥æ ¼éªŒè¯æ¨¡å¼")
        logger.info("=" * 50)

    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.error("è¯·æ£€æŸ¥:")
        logger.error("  1. é”™è¯¯æ—¥å¿—è¯¦æƒ…")
        logger.error("  2. ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")
        logger.error("  3. é…ç½®æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    print("ğŸš€ MalAPIé…ç½®åŒ–æ•°æ®å¤„ç†å±‚æµ‹è¯•")
    print("ğŸ“‹ ç‰¹æ€§: é›†ä¸­é…ç½®ç®¡ç†ã€ç¯å¢ƒå˜é‡æ”¯æŒã€æ™ºèƒ½è·¯å¾„æ£€æµ‹ã€è¯¦ç»†æŠ¥å‘Š")
    print("ğŸ“ æµ‹è¯•æ—¥å¿—å°†ä¿å­˜åˆ° test_data_processing.log")
    print("âš™ï¸  è¿è¡Œ 'python test_config.py' æŸ¥çœ‹å½“å‰é…ç½®")
    print("=" * 80)

    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    asyncio.run(main())